{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "import lxml.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url_pol = 'http://www.mof.gov.cn/zhengwuxinxi/zhengcefabu/index.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_links_from_main_url(base_url):\n",
    "    \"\"\"\n",
    "    Take a base_url and return a list of links on the page\n",
    "    \n",
    "    \"\"\"\n",
    "    #return a connection to some base_url\n",
    "    connection = urllib.request.urlopen(base_url)\n",
    "    #take the html object and get its dom structure\n",
    "    dom =  lxml.html.fromstring(connection.read())\n",
    "    #we need something to hold our urls\n",
    "    urls = []\n",
    "    #iterate over dom and get the href content\n",
    "    for path in dom.xpath('//a/@href'):\n",
    "        #if the item in the that object matches the pattern for a url append to our list\n",
    "        if path.startswith('http://') and path.endswith('.html'):\n",
    "            urls.append(path)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do some basic data cleaning\n",
    "def blob_gen(ps):\n",
    "    \"\"\"\n",
    "    Takes a Beautiful Soup result set object of 'p' elements and removes unicode object encoding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ps: BSoup Result Set list\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "    \"\"\"\n",
    "    blob = ''\n",
    "    for p in ps[1:]:\n",
    "        blob += str(p.text.replace('\\u3000', u' ').replace('\\xa0', u' '))\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_metadata(policy_url):\n",
    "    \"\"\"\n",
    "    Given a url returns a dictionary of key, value pairs of relevant metadata\n",
    "    \n",
    "    >>>get_metadata('http://www.some_url_from_some_website.html')\n",
    "    >>>{\n",
    "    ...'URL': 'http://www.some_url_from_some_website.html', \n",
    "    ...'title': 'Epic Surf on the North Shore', \n",
    "    ...'policy_id': 'Department of Stoke #234,\n",
    "    ...'date_published': 20161129, \n",
    "    ...'content': 'Stay stoked and be happy!'\n",
    "        }\n",
    "    \n",
    "    \"\"\"\n",
    "    #get html object for soup\n",
    "    html = requests.get(str(policy_url))\n",
    "    #intermediate object to Soupify and store the content from the page\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    #Parse the url and pull out the date of publish\n",
    "    date_published = re.search(r'([0-9]{8})', str(policy_url)).group()\n",
    "    #find the title \n",
    "    title = soup.find('td', class_='font_biao1').text.strip()\n",
    "    #get all the p elements where the text lives\n",
    "    ps = soup.find('div', class_='TRS_Editor').find_all('p')\n",
    "    #take the zeroth element where the policy id lives\n",
    "    policy_id = ps[0].text\n",
    "    \n",
    "    #get content\n",
    "    blob = blob_gen(ps)\n",
    "    \n",
    "    #construct our entry\n",
    "    entry = {\n",
    "    'URL': str(policy_url), \n",
    "     'title': title, \n",
    "     'policy_id': policy_id,\n",
    "     'date_published': date_published, \n",
    "    'content': blob\n",
    "          \n",
    "    }\n",
    "    \n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rem = re.search(r'([0-9]{8})', str(pol_urls[0]))\n",
    "date_published = rem.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup.find('td', class_='font_biao1').text.strip()\n",
    "ps = soup.find('div', class_='TRS_Editor').find_all('p')\n",
    "policy_nem = ps[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of urls\n",
    "type(ps[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_metadata_iter(base_url_pol):\n",
    "    for i, _ in enumerate(get_links_from_main_url(base_url_pol)):\n",
    "        print('Writing entry {} from url: {}'.format(i,_))\n",
    "        yield get_metadata(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = get_links_from_main_url(base_url_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pol.txt', 'w+') as f:\n",
    "    for url in urls:\n",
    "        entry = get_metadata(url)\n",
    "    f.writelines(entry)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode characters in position 1-3: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-79e3b606e27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'some_new_file.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_links_from_main_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url_pol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\justi\\Miniconda2\\envs\\py35\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\justi\\Miniconda2\\envs\\py35\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode characters in position 1-3: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import io\n",
    "with open('some_new_file.txt', 'w+') as f:\n",
    "    for url in urls:\n",
    "        json.dumps(get_metadata(url),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"URL\": \"http://gks.mof.gov.cn/zhengfuxinxi/guizhangzhidu/201611/t20161128_2467572.html\", \"date_published\": \"20161128\", \"policy_id\": \"财办库[2016]416号\", \"content\": \"党中央有关部门办公厅（室），国务院各部委、各直属机构办公厅（室），全国人大常委会办公厅秘书局，全国政协办公厅秘书局，高法院办公厅，高检院办公厅，各民主党派中央办公厅（室），有关人民团体办公厅（室）：  为简化优化中央预算单位变更政府采购方式和采购进口产品审批审核程序，提高审批审核工作效率，保障中央预算单位政府采购活动的顺利开展，现将有关事宜通知如下：  一、推行变更政府采购方式一揽子申报和批复  主管预算单位应加强本部门变更政府采购方式申报管理，定期归集所属预算单位申请项目，向财政部（国库司）一揽子申报，财政部（国库司）一揽子批复。归集的周期和频次由主管预算单位结合实际自行确定。时间紧急或临时增加的采购项目可单独申报和批复。  二、推行采购进口产品集中论证和统一报批  主管预算单位应按年度汇总所属预算单位的采购进口产品申请，组织专家集中论证后向财政部（国库司）申报，财政部（国库司）统一批复。时间紧急或临时增加的采购项目可单独申报和批复。  三、提高申报和审批审核效率  主管预算单位应完善内部管理规定和流程，明确时间节点和工作要求，及时做好所属预算单位变更政府采购方式和采购进口产品申报工作。对于中央预算单位变更政府采购方式和采购进口产品申请，财政部（国库司）实行限时办结制。对于申请理由不符合规定的项目，财政部（国库司）及时退回并告知原因；对于申请材料不完善和不符合规定的，财政部（国库司）一次性告知主管预算单位修改补充事项；对于符合规定的申请项目，财政部（国库司）自收到申请材料起５个工作日内完成批复。  中央预算单位变更政府采购方式和采购进口产品的其他事宜，按照《财政部关于印发＜中央预算单位变更政府采购方式审批管理办法＞的通知》（财库〔２０１５〕３６号）、《财政部关于印发＜政府采购进口产品管理办法＞的通知》（财库〔２００７〕１１９号）和《财政部关于完善中央单位政府采购预算管理和中央高校、科研院所科研仪器设备采购管理有关事项的通知》（财库〔２０１６〕１９４号）的有关规定执行。  本通知自２０１７年１月１日起执行。  财政部办公厅               ２０１６年１１月１８日\", \"title\": \"关于简化优化中央预算单位变更政府采购方式和采购进口产品审批审核有关事宜的通知\"}'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(get_metadata(get_links_from_main_url(base_url_pol)[0]), ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
